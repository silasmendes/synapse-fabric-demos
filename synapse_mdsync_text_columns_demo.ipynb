{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding MetadataSync mappings for text columns (Synapse Spark ↔ Serverless SQL)\n",
        "\n",
        "In this demo, we’ll explore how **MetadataSync** maps **String** and **Varchar** columns between **Synapse Spark** and **Synapse Serverless SQL pool**.\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "By the end of this demo, you’ll understand:\n",
        "\n",
        "- How text columns in Synapse Spark are mapped by MetadataSync to **Serverless SQL pool**.\n",
        "- The difference in behavior when using **Delta** vs **Parquet** tables.\n",
        "- Why certain mappings (like `VARCHAR(8000)` or `VARCHAR(MAX)`) are **by design**.\n",
        "- How to work around performance limitations caused by large text types."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Create database (if not exists)\n",
        "# ---------------------------\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS string_column_demo\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:24:43.4947346Z",
              "session_start_time": null,
              "execution_start_time": "2025-10-20T08:24:43.4958934Z",
              "execution_finish_time": "2025-10-20T08:24:50.5670799Z",
              "parent_msg_id": "deb918f8-0387-471a-95d0-da8435176dbb"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "DataFrame[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Create a Delta table in Spark\n",
        "\n",
        "Let’s start by creating a simple **Delta table** with three columns:  \n",
        "`id`, `event_time`, and `description`.\n",
        "\n",
        "Notice that the `description` column is defined as **String**."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Switch to the target database\n",
        "USE string_column_demo;\n",
        "\n",
        "-- Create a Delta table with 3 columns\n",
        "CREATE TABLE IF NOT EXISTS t1_delta_string (\n",
        "    id INT,\n",
        "    event_time TIMESTAMP,\n",
        "    description STRING\n",
        ")\n",
        "USING DELTA;\n",
        "\n",
        "-- Insert one record\n",
        "INSERT INTO t1_delta_string VALUES (1, current_timestamp(), 'She sells seashells by the seashore');"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 7,
              "statement_ids": [
                5,
                6,
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:26:14.5872316Z",
              "session_start_time": null,
              "execution_start_time": "2025-10-20T08:26:14.5876225Z",
              "execution_finish_time": "2025-10-20T08:27:07.9349683Z",
              "parent_msg_id": "8e47c818-87fa-4322-957f-f410829746c0"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observation\n",
        "\n",
        "- In **Serverless SQL pool**, there is **no “String” data type**.  \n",
        "- MetadataSync must choose an equivalent SQL type — in this case, it will use **VARCHAR(8000)**.\n",
        "\n",
        "You can confirm this by checking the table schema on Serverless SQL.\n",
        "\n",
        "This behavior is **by design** and consistent across **Synapse** and **Microsoft Fabric**."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Testing with a VARCHAR(n) column\n",
        "\n",
        "Let’s see what happens when we explicitly define the column as `VARCHAR(40)` in Spark instead of `String`.\n",
        "\n",
        "MetadataSync will map the `VARCHAR(40)` to `VARCHAR(160)` in Serverless SQL pool — that’s because it multiplies the size by 4.\n",
        "\n",
        "\n",
        "✅ **Summary**\n",
        "| Spark Type | Serverless Type | Notes |\n",
        "|-------------|-----------------|-------|\n",
        "| `String` | `VARCHAR(8000)` | Default mapping |\n",
        "| `VARCHAR(40)` | `VARCHAR(160)` | Multiplied by 4 |\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Switch to the target database\n",
        "USE string_column_demo;\n",
        "\n",
        "-- Create a Delta table with 3 columns\n",
        "CREATE TABLE IF NOT EXISTS t1_delta_varchar_40 (\n",
        "    id INT,\n",
        "    event_time TIMESTAMP,\n",
        "    description VARCHAR(40)\n",
        ")\n",
        "USING DELTA;\n",
        "\n",
        "-- Insert one record\n",
        "INSERT INTO t1_delta_varchar_40 VALUES (1, current_timestamp(), 'She sells seashells by the seashore');"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 10,
              "statement_ids": [
                8,
                9,
                10
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:29:15.1334334Z",
              "session_start_time": null,
              "execution_start_time": "2025-10-20T08:29:15.1338977Z",
              "execution_finish_time": "2025-10-20T08:29:23.2126146Z",
              "parent_msg_id": "15cb162d-84b3-46c6-9571-3cd558e7630a"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 10, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 - Switching to Parquet Tables\n",
        "\n",
        "Now, let’s test the same logic, but this time creating **Parquet** tables instead of Delta.\n",
        "\n",
        "In Parquet, you’ll notice that **regardless of using `String` or `VARCHAR(n)`**, MetadataSync will **always map** the column to **`VARCHAR(MAX)`** in Serverless SQL pool.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Switch to the target database\n",
        "USE string_column_demo;\n",
        "\n",
        "-- Create a Parquet table with 3 columns\n",
        "CREATE TABLE IF NOT EXISTS t1_parquet_string (\n",
        "    id INT,\n",
        "    event_time TIMESTAMP,\n",
        "    description STRING\n",
        ")\n",
        "USING PARQUET;\n",
        "\n",
        "-- Insert one record\n",
        "INSERT INTO t1_parquet_string VALUES (1, current_timestamp(), 'She sells seashells by the seashore');"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 13,
              "statement_ids": [
                11,
                12,
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:30:53.5287599Z",
              "session_start_time": null,
              "execution_start_time": "2025-10-20T08:30:53.5291653Z",
              "execution_finish_time": "2025-10-20T08:30:59.3427087Z",
              "parent_msg_id": "04a88509-819a-45b7-bee6-94d145d21def"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Switch to the target database\n",
        "USE string_column_demo;\n",
        "\n",
        "-- Create a Parquet table with 3 columns\n",
        "CREATE TABLE IF NOT EXISTS t1_parquet_varchar_40 (\n",
        "    id INT,\n",
        "    event_time TIMESTAMP,\n",
        "    description VARCHAR(40)\n",
        ")\n",
        "USING PARQUET;\n",
        "\n",
        "-- Insert one record\n",
        "INSERT INTO t1_parquet_varchar_40 VALUES (1, current_timestamp(), 'She sells seashells by the seashore');"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 16,
              "statement_ids": [
                14,
                15,
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:31:51.0267132Z",
              "session_start_time": null,
              "execution_start_time": "2025-10-20T08:31:51.0271497Z",
              "execution_finish_time": "2025-10-20T08:31:58.3045452Z",
              "parent_msg_id": "f6c981eb-a39d-43bc-ac41-ef8d942dc27f"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Creating Statistics error (Serverless SQL pool)\n",
        "\n",
        "Now switch to **Synapse Serverless SQL pool**.\n",
        "\n",
        "Let’s try to create **statistics** on the `description` column that was mapped as `VARCHAR(MAX)`.\n",
        "\n",
        "Run the following command in **SSMS** or **Synapse Studio (SQL On-Demand)**:\n",
        "\n",
        "```sql\n",
        "CREATE STATISTICS sDescription ON t1_parquet_varchar_40([description])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll see an error similar to:\n",
        "\n",
        "```\n",
        "Msg 2729, Level 16, State 1\n",
        "Column 'description' in table 't1_parquet' cannot be used in a statistics definition because its data type is a large object (LOB).\n",
        "\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "- Columns of type VARCHAR(MAX) are considered Large Objects (LOBs).\n",
        "- The Serverless SQL engine doesn’t support creating statistics on LOB columns.\n",
        "- Without statistics, the query optimizer has limited visibility into data distribution, which may cause suboptimal query plans."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 - Creating a SQL View (Serverless SQL pool)\n",
        "\n",
        "We’ll stay on the **Serverless SQL pool** side for this step.\n",
        "\n",
        "Since you can’t create statistics on columns defined as `VARCHAR(MAX)`,  a good workaround is to create a **custom view** that reads the same Parquet files using the `OPENROWSET` function —  but this time explicitly defining the column data types and lengths in the `WITH` clause.\n",
        "\n",
        "This approach lets you:\n",
        "\n",
        "- Control the data types manually (for example, use `VARCHAR(40)` instead of `VARCHAR(MAX)`).\n",
        "- Allow **statistics creation** on those columns.\n",
        "- Improve **query performance**.\n",
        "\n",
        "Example:\n",
        "\n",
        "```sql\n",
        "CREATE SCHEMA sch1\n",
        "GO\n",
        "CREATE VIEW sch1.v1_parquet_varchar_40\n",
        "AS\n",
        "SELECT\n",
        "    *\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        BULK 'https://<storage-path>.dfs.core.windows.net/<container-name>/synapse/workspaces/<workspace-name>/warehouse/<db-name>.db/<table-name>/**',\n",
        "        FORMAT = 'PARQUET'\n",
        "    )\n",
        "    WITH (\n",
        "        id INT,\n",
        "        event_time DATETIME,\n",
        "        description VARCHAR(40)\n",
        "    ) AS [result];\n",
        "GO\n",
        "\n",
        "SELECT * FROM sch1.v1_parquet_varchar_40\n",
        "```\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 - Challenge\n",
        "\n",
        "Alright, now for the **challenge**!  \n",
        "This exercise ties together everything we discussed in this notebook.\n",
        "\n",
        "Let’s go to the next cell that will create a **Delta table** that includes a column of type **String**.  \n",
        "It will also insert a single record containing a string with **10,000 characters**.\n",
        "\n",
        "Once you’ve executed it, switch to **Synapse Serverless SQL pool** and try to **SELECT** this new table.\n",
        "\n",
        "You’ll notice that the query **fails with an error**.\n",
        "\n",
        "> **Your mission:**\n",
        ">\n",
        "> 1. Identify the **root cause** of this error.  \n",
        "> 2. Propose a **solution or workaround** to fix it.  \n",
        "> 3. (Bonus) Explain *why* this behavior happens when using certain data types across Spark and Serverless."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Switch to the target database\n",
        "USE string_column_demo;\n",
        "\n",
        "-- Create a Delta table with 3 columns\n",
        "CREATE TABLE IF NOT EXISTS t1_delta_varchar_string_challenge (\n",
        "    id INT,\n",
        "    event_time TIMESTAMP,\n",
        "    description STRING\n",
        ")\n",
        "USING DELTA;\n",
        "\n",
        "-- Insert one record with a string with 10K caracters\n",
        "INSERT INTO t1_delta_varchar_string_challenge \n",
        "VALUES (1, current_timestamp(), repeat('A', 10000));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 19,
              "statement_ids": [
                17,
                18,
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:37:39.2912994Z",
              "session_start_time": null,
              "execution_start_time": "2025-10-20T08:37:39.2917676Z",
              "execution_finish_time": "2025-10-20T08:37:45.0177086Z",
              "parent_msg_id": "b368cc34-9023-4737-8a67-5ab471c220b1"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s the end of the demo — great job if you made it this far! 👏  \n",
        "Feel free to DROP the `string_column_demo` database:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DROP DATABASE IF EXISTS string_column_demo CASCADE\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkplay01",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-10-20T08:18:27.5242243Z",
              "session_start_time": "2025-10-20T08:18:27.52519Z",
              "execution_start_time": "2025-10-20T08:23:10.4270421Z",
              "execution_finish_time": "2025-10-20T08:23:33.6870755Z",
              "parent_msg_id": "259b597b-243f-42e4-8bc2-0c695d8210e2"
            },
            "text/plain": "StatementMeta(sparkplay01, 4, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "DataFrame[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}